{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684e9670-45ce-4d38-aed3-d638ada7e5df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/d3/e2/17bad124c8dd2aa0a3062e44992eb34c282379450ebbe6fdb6b96aa3c907/gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Using cached smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Using cached gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "Using cached smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad23c1f6-eada-4fb0-9fc1-a13cc7a988f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.599469496\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Load the pre-trained Word2Vec model (Google News vectors)\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Define a function to convert text to document embeddings\n",
    "def text_to_embeddings(text, model):\n",
    "    words = text.split()\n",
    "    valid_words = [word for word in words if word in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(300)  # Return a zero vector if no valid words found\n",
    "    return np.mean([model[word] for word in valid_words], axis=0)\n",
    "\n",
    "# Convert text data to document embeddings\n",
    "X_embeddings = np.array([text_to_embeddings(doc, word2vec_model) for doc in X])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.9f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882a7762-4775-4ae1-b147-d57b6009bb7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.432891247\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Load GloVe embeddings into a dictionary\n",
    "glove_embeddings = {}\n",
    "glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Define a function to convert text to document embeddings\n",
    "def text_to_embeddings(text, embeddings):\n",
    "    words = text.split()\n",
    "    valid_words = [word for word in words if word in embeddings]\n",
    "    if not valid_words:\n",
    "        return np.zeros(50)  # Return a zero vector if no valid words found\n",
    "    return np.mean([embeddings[word] for word in valid_words], axis=0)\n",
    "\n",
    "# Convert text data to document embeddings\n",
    "X_embeddings = np.array([text_to_embeddings(doc, glove_embeddings) for doc in X])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.9f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1110bba8-e75b-47fb-b61d-c108a04803a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.595755968\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Load FastText embeddings into a dictionary\n",
    "fasttext_embeddings = {}\n",
    "fasttext_file = 'wiki-news-300d-1M.vec'\n",
    "\n",
    "with open(fasttext_file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().split(' ')\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        fasttext_embeddings[word] = vector\n",
    "\n",
    "# Define a function to convert text to document embeddings\n",
    "def text_to_embeddings(text, embeddings):\n",
    "    words = text.split()\n",
    "    valid_words = [word for word in words if word in embeddings]\n",
    "    if not valid_words:\n",
    "        return np.zeros(300)  # Return a zero vector if no valid words found\n",
    "    return np.mean([embeddings[word] for word in valid_words], axis=0)\n",
    "\n",
    "# Convert text data to document embeddings\n",
    "X_embeddings = np.array([text_to_embeddings(doc, fasttext_embeddings) for doc in X])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.9f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee0902-ca30-4d67-84b4-39229b99c910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
